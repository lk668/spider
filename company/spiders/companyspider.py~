#!/usr/bin/python
# -*- coding:utf-8 -*-

# from scrapy.contrib.spiders import  CrawlSpider,Rule

from scrapy.spiders import Spider
from scrapy.http import Request
from scrapy.selector import Selector
from company.items import CompanyItem

class CompanySpider(Spider):
    """爬虫CompanySpider"""

    name = "Company"

    #减慢爬取速度 为1s
    download_delay = 1
    allowed_domains = ["http://ecp.sgcc.com.cn/"]
    start_urls = [
        #第一篇文章地址
        "http://ecp.sgcc.com.cn/news_list.jsp?site=global&column_code=014002003&company_id=01&news_name=all&pageNo=1",
        #"http://ecp.sgcc.com.cn/news_list.jsp?site=global&column_code=014002003&company_id=01&news_name=all&pageNo=2",
        #"http://ecp.sgcc.com.cn/news_list.jsp?site=global&column_code=014002003&company_id=01&news_name=all&pageNo=3"
    ]
    for i in range(2,3):
        start_urls.append("http://ecp.sgcc.com.cn/news_list.jsp?site=global&column_code=014002003&company_id=01&news_name=all&pageNo=" + str(i))
    def parse(self, response):
        sel = Selector(response)
        news_url=[]
        #items = []
        #获得文章url和标题
        item = CompanyItem()
        urls = sel.xpath('//div[@class="titleList"]/ul[@class="newslist01"]/li/a/@onclick').extract()
        for url in urls:
            news_url.append("http://ecp.sgcc.com.cn/html/news/014002003/"+url[29:-3]+".html")
        #print news_url
        #news_name = sel.xpath('//head/title/text()').extract()
        news_name = sel.xpath('//div[@class="titleList"]/ul[@class="newslist01"]/li/a/@title').extract()
        for i in range(0,len(news_name)):
            item['news_name'] = news_name[i].encode('utf-8')
            item['news_url'] = news_url[i].encode('utf-8')
            yield item
        '''
        urls = sel.xpath('//div[@class="page"]/a/@href').extract()
        for url in urls:
            url = "http://ecp.sgcc.com.cn/" + url
            print url
            print "############"
            yield Request(url, callback=self.parse)
        '''
